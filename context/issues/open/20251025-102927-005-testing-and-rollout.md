# Continuous Monitoring Testing and Rollout

**Type:** feature
**Initiative:** End-to-end trader workflow implementation
**Created:** 2025-10-25 10:29:27

## Context

All continuous monitoring components are implemented (candle close events, setup monitoring, position management, workflow state). Now we need comprehensive testing and gradual rollout to ensure the system works reliably in production.

**Goal:** Validate end-to-end flows, measure performance, catch edge cases, and safely deploy to production users.

## Linked Items

- Part of: `context/issues/open/20251025-102927-000-PROJECT-continuous-monitoring-system.md`
- Depends on: All other sub-issues (001-004)
- Blocks: Production launch of continuous monitoring

## Progress

*Track progress here*

## Spec

### Testing Strategy

#### Phase 1: Unit Tests (Already Done in Sub-Issues)
Each component should have unit tests:
- ✅ Candle close event emission
- ✅ PostgreSQL NOTIFY triggers
- ✅ Monitoring workflow creation
- ✅ Position management workflow creation
- ✅ Deduplication logic
- ✅ Error handling

#### Phase 2: Integration Tests

**2.1 Setup Monitoring Integration Test**

```typescript
describe('Setup Monitoring End-to-End', () => {
  it('should monitor signal with decision=wait until decision changes', async () => {
    // 1. Create test trader (auto_analyze_signals=true)
    const trader = await createTestTrader({
      auto_analyze_signals: true,
      filter: { interval: '1m' }
    });

    // 2. Create signal (triggers initial analysis)
    const signal = await createTestSignal({
      trader_id: trader.id,
      symbol: 'BTCUSDT'
    });

    // 3. Wait for initial analysis
    await waitForAnalysis(signal.id);
    let analysis = await getLatestAnalysis(signal.id);

    // 4. Verify initial analysis exists
    expect(analysis).toBeDefined();
    expect(analysis.decision).toBe('wait');  // Mock to return "wait"

    // 5. Wait for next candle close (1 minute)
    await waitForCandleClose('BTCUSDT', '1m');

    // 6. Verify second analysis was created
    const analyses = await getAllAnalyses(signal.id);
    expect(analyses.length).toBe(2);

    // 7. Verify second analysis has candle close timestamp
    const secondAnalysis = analyses[1];
    expect(isAtCandleClose(secondAnalysis.created_at)).toBe(true);

    // 8. Mock decision change to "enter_trade"
    mockNextAnalysisDecision('enter_trade');

    // 9. Wait for next candle close
    await waitForCandleClose('BTCUSDT', '1m');

    // 10. Verify monitoring stopped (no new analyses)
    await waitForCandleClose('BTCUSDT', '1m');  // One more candle
    const finalAnalyses = await getAllAnalyses(signal.id);
    expect(finalAnalyses.length).toBe(3);  // No 4th analysis
  });

  it('should stop monitoring after max reanalyses', async () => {
    // Configure max reanalyses = 3
    // Create signal with decision="wait"
    // Wait for 3 candle closes
    // Verify 4 total analyses (1 initial + 3 reanalyses)
    // Wait for another candle close
    // Verify no 5th analysis (monitoring stopped)
    // Verify signal status = 'expired'
  });
});
```

**2.2 Position Management Integration Test**

```typescript
describe('Position Management End-to-End', () => {
  it('should manage position at each candle until closed', async () => {
    // 1. Create trader
    const trader = await createTestTrader({
      filter: { interval: '1m' }
    });

    // 2. Open test position
    const position = await createTestPosition({
      trader_id: trader.id,
      symbol: 'BTCUSDT',
      side: 'long',
      entry_price: 50000,
      contracts: 1,
      status: 'open'
    });

    // 3. Wait for first candle close
    await waitForCandleClose('BTCUSDT', '1m');

    // 4. Verify management decision created
    let decisions = await getManagementDecisions(position.id);
    expect(decisions.length).toBe(1);
    expect(decisions[0].action).toBe('hold');  // Mock decision

    // 5. Mock next decision: adjust_sl
    mockNextManagementDecision('adjust_sl', { newStopLoss: 49500 });

    // 6. Wait for next candle close
    await waitForCandleClose('BTCUSDT', '1m');

    // 7. Verify SL was adjusted
    const updatedPosition = await getPosition(position.id);
    expect(updatedPosition.stop_loss).toBe(49500);

    // 8. Close position
    await closePosition(position.id);

    // 9. Wait for next candle close
    await waitForCandleClose('BTCUSDT', '1m');

    // 10. Verify no new management decision (stopped)
    decisions = await getManagementDecisions(position.id);
    expect(decisions.length).toBe(2);  // Only 2, not 3
  });
});
```

#### Phase 3: Load Testing

**3.1 Multiple Simultaneous Monitors**

```go
func TestMultipleMonitorsPerformance(t *testing.T) {
	// Create 50 signals, all with decision="wait"
	// All monitored on same symbol/interval (BTCUSDT-1m)
	// Trigger candle close
	// Verify all 50 get analyzed within 10 seconds
	// Check memory usage (should be reasonable)
	// Check no race conditions
}
```

**3.2 Token Usage Validation**

```typescript
it('should match token usage projections', async () => {
  // Simulate 1 day of activity
  // 100 signals created
  // ~20% get decision="wait" (20 signals monitored)
  // Average 3 reanalyses each
  // 10 positions opened
  // Average 8 candles each

  const totalTokens = await measureTokenUsage();

  // Should be ~264k tokens/day (from projections)
  expect(totalTokens).toBeCloseTo(264000, -3);  // Within 1k
});
```

#### Phase 4: Error Scenario Testing

**4.1 llm-proxy Failures**

```go
func TestLLMProxyFailure(t *testing.T) {
	// Mock llm-proxy to return 500 errors
	// Trigger candle close
	// Verify error logged
	// Verify consecutive_errors incremented
	// After 5 failures, verify workflow disabled
	// Verify user notified
}
```

**4.2 WebSocket Reconnection**

```go
func TestWebSocketReconnect(t *testing.T) {
	// Start monitoring
	// Disconnect WebSocket
	// Wait for reconnection
	// Verify candle close events resume
	// Verify no duplicate processing
}
```

**4.3 Database Connection Loss**

```go
func TestDatabaseReconnect(t *testing.T) {
	// Start monitoring
	// Simulate database connection loss
	// Trigger candle close (should queue)
	// Restore database connection
	// Verify queued analyses processed
}
```

### Rollout Plan

#### Stage 1: Dev Testing (3 days)

**Environment:** Development database, test traders only

**Scope:**
- Create 10 test traders with auto_analyze_signals=true
- Generate test signals (manual insertion)
- Monitor for 3 days of continuous operation
- Metrics to track:
  - Analyses per day
  - Token usage
  - Error rate
  - Average latency
  - Duplicate candle processing (should be 0)

**Success Criteria:**
- Zero duplicate analyses (deduplication working)
- <1% error rate
- Average latency <5s per analysis
- Token usage within 10% of projections

#### Stage 2: Canary Deployment (1 week)

**Environment:** Production database

**Scope:**
- Enable for 1 Elite user (willing volunteer)
- Monitor their traders with auto_analyze_signals=true
- Daily check-ins for feedback

**Metrics:**
- Analysis quality (user feedback)
- System stability
- Token costs (actual vs projected)
- User satisfaction

**Rollback Trigger:**
- >5% error rate
- User complaints about quality
- Token costs >2x projections
- System instability (crashes, memory leaks)

#### Stage 3: Gradual Rollout (1 week)

**Day 1-2:** 10% of Elite users (randomly selected)
**Day 3-4:** 25% of Elite users
**Day 5-6:** 50% of Elite users
**Day 7:** 100% of Elite users

**Each stage:**
- Monitor metrics for 24-48 hours before expanding
- Check error rates, token usage, user feedback
- Ready to rollback at any stage

#### Stage 4: Full Production (Ongoing)

**All Elite users with auto_analyze_signals=true**

**Continuous monitoring:**
- Error rates (alert if >1%)
- Token usage (alert if >125% of projections)
- Latency (alert if >10s average)
- User complaints

**Weekly reviews:**
- Analysis quality assessment
- Token cost analysis
- Performance optimization opportunities
- User feedback synthesis

### Monitoring & Observability

**1. Metrics Dashboard**

Key metrics to track:
```
- Active monitoring workflows (count)
- Active position management workflows (count)
- Analyses per hour (rate)
- Token usage per hour (cost)
- Error rate (percentage)
- Average analysis latency (seconds)
- Duplicate candle count (should be 0)
- Workflow auto-disabled count (errors)
```

**2. Alerting**

Set up alerts for:
```
- Error rate >1% (warning), >5% (critical)
- Average latency >10s (warning), >20s (critical)
- Token usage >125% of projections (warning)
- Any duplicate candle processing (critical)
- Workflow auto-disabled (informational)
```

**3. Logging**

Ensure logs capture:
```
[MonitoringEngine] Workflow started: signal_id, symbol, interval
[MonitoringEngine] Candle closed: symbol-interval, monitors_count
[MonitoringEngine] Analysis complete: signal_id, decision, confidence, tokens
[MonitoringEngine] Workflow stopped: signal_id, reason
[MonitoringEngine] Error: signal_id, error_message, consecutive_errors
```

**4. Braintrust Dashboard**

Monitor in Braintrust:
- Trace counts for analyze-signal and manage-position operations
- Token usage by operation type
- Confidence score distributions
- Decision type distributions (wait vs enter vs bad_setup)

### Rollback Plan

**If critical issues discovered:**

```bash
# 1. Immediately disable new workflow creation
UPDATE traders SET auto_analyze_signals = false
WHERE user_id IN (SELECT id FROM user_profiles WHERE subscription_tier = 'elite');

# 2. Stop active monitoring workflows
UPDATE workflow_schedules SET is_active = false WHERE is_active = true;

# 3. Investigate and fix issue

# 4. Re-enable gradually once fixed
```

### Files to Create

**Testing:**
- `backend/go-screener/test/integration/monitoring_test.go`
- `backend/go-screener/test/integration/position_management_test.go`
- `backend/go-screener/test/load/multi_monitor_test.go`
- Frontend integration tests (if UI added)

**Monitoring:**
- `scripts/monitoring-metrics.sh` (collect metrics)
- `scripts/rollback-monitoring.sql` (emergency rollback)
- Grafana dashboard config (if using Grafana)

### Success Criteria

- [ ] All integration tests pass
- [ ] Load tests pass (50 simultaneous monitors)
- [ ] Error scenario tests pass
- [ ] Dev testing: 3 days stable operation
- [ ] Canary: 1 week with 1 user, positive feedback
- [ ] Gradual rollout: Each stage 24-48 hours stable
- [ ] Full production: 1 week stable, <1% error rate
- [ ] Token usage within 25% of projections
- [ ] User satisfaction >4/5 (if surveyed)
- [ ] Zero critical bugs in production

### Effort Estimate

**3-5 days testing + 2 weeks rollout**

**Testing (3-5 days):**
- Day 1: Integration tests
- Day 2: Load testing, error scenarios
- Day 3: Dev environment testing setup
- Days 4-5: Run dev testing, collect metrics

**Rollout (2 weeks):**
- Week 1: Canary (1 user)
- Week 2: Gradual rollout (10% → 100%)

**Ongoing:** Continuous monitoring and optimization
